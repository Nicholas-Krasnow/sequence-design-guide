{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDpubqpwrK6",
        "outputId": "85621a60-6cd0-40c1-d47d-556ab3aa4b41"
      },
      "outputs": [],
      "source": [
        "!pip install biopython\n",
        "import Bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zi38q5MK2MIj"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft_Ur2Vm3KtG",
        "outputId": "9fa5bf74-88c9-4fd5-ba40-5f77bb489200"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "drive.mount('/content/google_drive')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd_y37pBjXpG"
      },
      "source": [
        "Move into the fastas folder to import the list of designed sequences. Custom MSA should also be in this directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u3sDgCrB2pH3"
      },
      "outputs": [],
      "source": [
        "os.chdir('google_drive/MyDrive/BoNT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpMtlQlipu9g"
      },
      "source": [
        "Import your fasta file containing all your sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3XaNdNS4sfH",
        "outputId": "648de2b3-5fda-415e-c7f5-c7826dacfcba"
      },
      "outputs": [],
      "source": [
        "parsed = SeqIO.parse('deduplicated_designs_renamed.fasta','fasta')\n",
        "i = 1\n",
        "seqsDict = {}\n",
        "for x in parsed:\n",
        "  seqsDict[str(i)] = str(x.seq)\n",
        "  i += 1\n",
        "seqsDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaqAdCHl3Zqc"
      },
      "source": [
        "Run in custom MSA mode with the WT MSA specified in the current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A9SHLhTtz_T",
        "outputId": "2b2b7c36-2ffb-4b91-fef5-a995a26c1777"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "def add_hash(x,y,i):\n",
        "  return x+\"_\"+str(i)+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "tmp = []\n",
        "for i,sequenceItem in enumerate(seqsDict):\n",
        "  query_sequence = seqsDict[sequenceItem] #@param {type:\"string\"}\n",
        "  query_sequence = query_sequence.replace(\"/\", \":\")\n",
        "  jobname = \"cbe6b\" #@param {type:\"string\"}\n",
        "  query_sequence = \"\".join(query_sequence.split())\n",
        "  #print(i)\n",
        "  #print(query_sequence)\n",
        "\n",
        "  basejobname = \"\".join(jobname.split())\n",
        "  basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "  jobname = add_hash(basejobname, query_sequence,i)\n",
        "  tmp.append(jobname)\n",
        "\n",
        "print(datetime.now())\n",
        "print(tmp)\n",
        "print(len(tmp) == len(set(tmp)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I7MhyVAy4_h6",
        "outputId": "f1dd8bd1-1a59-4163-b94c-dc3eef1caa7c"
      },
      "outputs": [],
      "source": [
        "#@title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
        "%%time\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "def add_hash(x,y,i):\n",
        "  return x+\"_\"+str(i)+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "#template_mode = \"custom\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
        "\n",
        "for i,sequenceItem in enumerate(seqsDict):\n",
        "  print(\"------------\" + str(i) + \"------------\")\n",
        "  print(datetime.now())\n",
        "\n",
        "  query_sequence = seqsDict[sequenceItem] #@param {type:\"string\"}\n",
        "  #@markdown  - Use `:` to specify inter-protein chainbreaks for **modeling complexes** (supports homo- and hetro-oligomers). For example **PI...SK:PI...SK** for a homodimer\n",
        "  jobname = \"bont\" #@param {type:\"string\"}\n",
        "  # number of models to use\n",
        "  num_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n",
        "  #@markdown - specify how many of the top ranked structures to relax using amber\n",
        "  template_mode = \"custom\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
        "  template_name = \"1xtg.pdb\" # @param {\"type\":\"string\",\"placeholder\":\"1xtg.pdb\"}\n",
        "  #@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
        "\n",
        "  use_amber = num_relax > 0\n",
        "\n",
        "  # remove whitespaces\n",
        "  query_sequence = query_sequence.replace(\"/\", \":\")\n",
        "  query_sequence = \"\".join(query_sequence.split())\n",
        "\n",
        "  basejobname = \"\".join(jobname.split())\n",
        "  basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "  jobname = add_hash(basejobname, query_sequence,i)\n",
        "\n",
        "  # check if directory with jobname exists\n",
        "  def check(folder):\n",
        "    if os.path.exists(folder):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "  if not check(jobname):\n",
        "    n = 0\n",
        "    while not check(f\"{jobname}_{n}\"): n += 1\n",
        "    jobname = f\"{jobname}_{n}\"\n",
        "\n",
        "  # make directory to save results\n",
        "  os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "  # save queries\n",
        "  queries_path = os.path.join(jobname, f\"{jobname}.csv\")\n",
        "  with open(queries_path, \"w\") as text_file:\n",
        "    text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "\n",
        "  if template_mode == \"pdb100\":\n",
        "    use_templates = True\n",
        "    custom_template_path = None\n",
        "  elif template_mode == \"custom\":\n",
        "    custom_template_path = os.path.join(jobname,f\"template\")\n",
        "    os.makedirs(custom_template_path, exist_ok=True)\n",
        "    #uploaded = files.upload()\n",
        "    !cp \"/content/google_drive/MyDrive/BoNT/{template_name}\" \"{jobname}/template/{template_name}\"\n",
        "    print('Template file copied to:', os.path.join(jobname, 'template', template_name))\n",
        "    use_templates = True\n",
        "  else:\n",
        "    custom_template_path = None\n",
        "    use_templates = False\n",
        "\n",
        "  print(\"jobname\",jobname)\n",
        "  print(\"sequence\",query_sequence)\n",
        "  print(\"length\",len(query_sequence.replace(\":\",\"\")))\n",
        "\n",
        "  #@title Install dependencies\n",
        "  #%%time\n",
        "  import os\n",
        "  USE_AMBER = use_amber\n",
        "  USE_TEMPLATES = use_templates\n",
        "  PYTHON_VERSION = python_version\n",
        "\n",
        "  if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "    print(\"installing colabfold...\")\n",
        "    os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "    if os.environ.get('TPU_NAME', False) != False:\n",
        "      os.system(\"pip uninstall -y jax jaxlib\")\n",
        "      os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "    os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "  if USE_AMBER or USE_TEMPLATES:\n",
        "    if not os.path.isfile(\"CONDA_READY\"):\n",
        "      print(\"installing conda...\")\n",
        "      os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "      os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "      os.system(\"mamba config --set auto_update_conda false\")\n",
        "      os.system(\"touch CONDA_READY\")\n",
        "\n",
        "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "    print(\"installing hhsuite and amber...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "    os.system(\"touch HH_READY\")\n",
        "    os.system(\"touch AMBER_READY\")\n",
        "  else:\n",
        "    if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "      print(\"installing hhsuite...\")\n",
        "      os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "      os.system(\"touch HH_READY\")\n",
        "    if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "      print(\"installing amber...\")\n",
        "      os.system(f\"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "      os.system(\"touch AMBER_READY\")\n",
        "\n",
        "  #@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
        "  msa_mode = \"single_sequence\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "  pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
        "  #@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
        "\n",
        "  # decide which a3m to use\n",
        "  if \"mmseqs2\" in msa_mode:\n",
        "    a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "  elif msa_mode == \"custom\":\n",
        "    #path to custom MSA: a3m generated for WT BoNT/X prediction\n",
        "    a3m_file = \"X_MSA.a3m\"\n",
        "    if not os.path.isfile(a3m_file):\n",
        "      custom_msa_dict = files.upload()\n",
        "      custom_msa = list(custom_msa_dict.keys())[0]\n",
        "      header = 0\n",
        "      import fileinput\n",
        "      for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "        if line.startswith(\">\"):\n",
        "          header = header + 1\n",
        "        if not line.rstrip():\n",
        "          continue\n",
        "        if line.startswith(\">\") == False and header == 1:\n",
        "          query_sequence = line.rstrip()\n",
        "        print(line, end='')\n",
        "\n",
        "      os.rename(custom_msa, a3m_file)\n",
        "      queries_path=a3m_file\n",
        "      print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "  else:\n",
        "    a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "    with open(a3m_file, \"w\") as text_file:\n",
        "      text_file.write(\">1\\n%s\" % query_sequence)\n",
        "\n",
        "  #@markdown ### Advanced settings\n",
        "  model_type = \"alphafold2_multimer_v3\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\"]\n",
        "  #@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
        "  #@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
        "  num_recycles = \"6\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
        "  #@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
        "  recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "  #@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
        "  pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
        "  #@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
        "\n",
        "\n",
        "  #@markdown #### Sample settings\n",
        "  #@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
        "  #@markdown -  decrease `max_msa` to increase uncertainity\n",
        "  max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
        "  num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "  use_dropout = False #@param {type:\"boolean\"}\n",
        "\n",
        "  num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "  recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "  if max_msa == \"auto\": max_msa = None\n",
        "\n",
        "  #@markdown #### Save settings\n",
        "  save_all = False #@param {type:\"boolean\"}\n",
        "  save_recycles = False #@param {type:\"boolean\"}\n",
        "  save_to_google_drive = False #@param {type:\"boolean\"}\n",
        "  #@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
        "  dpi = 200 #@param {type:\"integer\"}\n",
        "  #@markdown - set dpi for image resolution\n",
        "\n",
        "  if save_to_google_drive:\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    print(\"You are logged into Google Drive and are good to go!\")\n",
        "\n",
        "  #@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form.\n",
        "\n",
        "  #@title Run Prediction\n",
        "  display_images = False #@param {type:\"boolean\"}\n",
        "\n",
        "  import sys\n",
        "  import warnings\n",
        "  warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "  from Bio import BiopythonDeprecationWarning\n",
        "  warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "  from pathlib import Path\n",
        "  from colabfold.download import download_alphafold_params, default_data_dir\n",
        "  from colabfold.utils import setup_logging\n",
        "  from colabfold.batch import get_queries, run, set_model_type\n",
        "  from colabfold.plot import plot_msa_v2\n",
        "\n",
        "  import os\n",
        "  import numpy as np\n",
        "  try:\n",
        "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "  except:\n",
        "    K80_chk = \"0\"\n",
        "    pass\n",
        "  if \"1\" in K80_chk:\n",
        "    print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "    if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "      del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "    if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "      del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "  from colabfold.colabfold import plot_protein\n",
        "  from pathlib import Path\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # For some reason we need that to get pdbfixer to import\n",
        "  if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
        "      sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
        "\n",
        "  def input_features_callback(input_features):\n",
        "    if display_images:\n",
        "      plot_msa_v2(input_features)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "  def prediction_callback(protein_obj, length,\n",
        "                          prediction_result, input_features, mode):\n",
        "    model_name, relaxed = mode\n",
        "    if not relaxed:\n",
        "      if display_images:\n",
        "        fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "  result_dir = jobname\n",
        "  log_filename = os.path.join(jobname,\"log.txt\")\n",
        "  if not os.path.isfile(log_filename) or 'logging_setup' not in globals():\n",
        "    setup_logging(Path(log_filename))\n",
        "    logging_setup = True\n",
        "\n",
        "  queries, is_complex = get_queries(queries_path)\n",
        "  model_type = set_model_type(is_complex, model_type)\n",
        "\n",
        "  if \"multimer\" in model_type and max_msa is not None:\n",
        "    use_cluster_profile = False\n",
        "  else:\n",
        "    use_cluster_profile = True\n",
        "\n",
        "  download_alphafold_params(model_type, Path(\".\"))\n",
        "  results = run(\n",
        "      queries=queries,\n",
        "      result_dir=result_dir,\n",
        "      use_templates=use_templates,\n",
        "      custom_template_path=custom_template_path,\n",
        "      num_relax=num_relax,\n",
        "      msa_mode=msa_mode,\n",
        "      model_type=model_type,\n",
        "      num_models=5,\n",
        "      num_recycles=num_recycles,\n",
        "      recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
        "      num_seeds=num_seeds,\n",
        "      use_dropout=use_dropout,\n",
        "      model_order=[1,2,3,4,5],\n",
        "      is_complex=is_complex,\n",
        "      data_dir=Path(\".\"),\n",
        "      keep_existing_results=False,\n",
        "      rank_by=\"auto\",\n",
        "      pair_mode=pair_mode,\n",
        "      pairing_strategy=pairing_strategy,\n",
        "      stop_at_score=float(100),\n",
        "      prediction_callback=prediction_callback,\n",
        "      dpi=dpi,\n",
        "      zip_results=False,\n",
        "      save_all=save_all,\n",
        "      max_msa=max_msa,\n",
        "      use_cluster_profile=use_cluster_profile,\n",
        "      input_features_callback=input_features_callback,\n",
        "      save_recycles=save_recycles,\n",
        "      user_agent=\"colabfold/google-colab-main\",\n",
        "  )\n",
        "  results_zip = f\"{jobname}.result.zip\"\n",
        "  os.system(f\"zip -r {results_zip} {jobname}\")\n",
        "\n",
        "  #@title Package and download results\n",
        "  #@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "  if msa_mode == \"custom\":\n",
        "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "  files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "  if save_to_google_drive == True and drive:\n",
        "    uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "    uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "    uploaded.Upload()\n",
        "    print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
